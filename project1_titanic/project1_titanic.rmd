---
title: "Project 1 - Titanic"
author: "Nick Tedesco"
date: "2022-12-18"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Package and Data Loading

```{r, include = FALSE}
#| label: packages

library(tidyverse)
library(caret)
library(dplyr)
library(ggplot2)
```

```{r}
#| label: train data

train <- read.csv('train.csv', na = "")

glimpse(train)
```

```{r}
#| label: test data

test <- read.csv('test.csv', na = "")

glimpse(test)
```

Our initial inspection of the data reveals a few interesting points...
- there are a few variables which are seemingly useless for our analysis (ticket, cabin, name)
- however, we may be able to extract an individual's title from their name, or perhaps their relative location on the ship from their cabin
- we have to deal with missing values

Let's start by preprocessing our data. This will include dealing with missing values, looking for and addressing outliers, and some feature engineering.

## Preprocessing

We'll start by joining our training and testing data for joint preprocessing, and dropping the ticket variable. 

```{r}
#| label: train test join

## define train/test identification variable
train$train <- "yes"
test$train <- "no"

## extract outcome variable from training dataset
train.Y <- train$Survived
train <- train %>% select(-Survived)

data <- rbind(train, test)
```

```{r}
#| label: remove ticket variable

data <- data %>% select(-Ticket)
```

### Missing Values

Next, let's take a look at the distribution of missing values. 

```{r}
#| label: missing values

colSums(is.na(data)) / nrow(data)
```

The proportion of missing values for Cabin is too high for imputation, so we will simply remove this variable. 

```{r}
#| label: drop Cabin variable

data <- data %>% select(-Cabin)
```

Fare and Embarked have extremely low missing value proportions. As long as they aren't in the testing dataset, we can simply drop these rows. 

```{r}
#| label: closer look at Fare and Embarked

data[is.na(data$Fare) | is.na(data$Embarked), ]
```

As shown by the "train" indicator variable, the two rows that contain missing values for Embarked are in the training set. Therefore, we can drop these rows. In order to drop the entire row, we have to join back the training set outcome (Survived) before we drop these rows. 

```{r}
#| label: drop rows with missing Embarked

train <- data[data$train == "yes", ]
test <- data[data$train == "no", ]

train$Survived <- train.Y

train <- train[!is.na(train$Embarked), ]

train.Y <- train$Survived

train <- train |> 
  select(-Survived)

data <- rbind(train, test)
```

Unfortunately, the row with a missing value for Fare is in the test dataset - therefore, we cannot drop it. Instead, we will perform missing value imputation (here, we'll use the median to avoid the influence of outliers). Since we have a ticket class variable (Pclass), it would make sense to group median fare by ticket class. 

```{r}
#| label: missing value imputation for Fare

unique_pclass <- unique(data$Pclass)

for(class in unique_pclass){
  data[is.na(data$Fare) & data$Pclass == class, "Fare"] <- median(data[data$Pclass == class, "Fare"], na.rm = TRUE)
}
```

Finally, let's consider age. Even if all the observations with missing values for age were within the training set, dropping these rows would result in losing a large chunk of our data. Therefore, we will perform missing value imputation for age. 

The overall mean or median of age is likely a poor estimate of a given passenger's age. We have the Name variable, which seems to contain each passenger's title (ex: Master, Mr, Dr). Therefore, it makes sense to group our age imputation by title. Here, we will use the mean (instead of the median) because the groupings will likely contain a small number of rows, which would suggest that the median might be a poor representation of a given group's age. 

Let's start by creating a Title variable. We will use a collapsed version to limit the number of unique titles.

```{r}
#| label: creating Title variable

## use gsub to take advantage of the fact that each title ends with a period
data$Title <- gsub("\\..*|.*, |the ", "", data$Name)

## collapse titles 
data <- data |> 
  mutate(Title = 
           case_when(
             Title %in% c("Major", "Sir", "Col", "Don", "Jonkheer", "Capt") ~ "Mr", 
             Title %in% c("Mlle", "Mme", "Ms") ~ "Miss", 
             Title %in% c("Lady", "Countess", "Dona") ~ "Mrs", 
             TRUE ~ Title
           )
  )
```

Now, let's perform missing value imputation by title group.

```{r}
#| label: missing value imputation for Age

unique_title <- unique(data$Title)

for(title in unique_title){
  data[is.na(data$Age) & data$Title == title, "Age"] <- mean(data[data$Title == title, "Age"], na.rm = TRUE)
}
```

Finally, let's check to make sure we've addressed all the missing values. 

```{r}
#| label: missing values 2 

colSums(is.na(data)) / nrow(data)
```

### Checking for Outliers

Next, we will check our continuous variables (Age, Fare) for any potential outliers.

```{r}
#| label: check for outliers

data |> 
  pivot_longer(cols = c(Age, Fare), names_to = "Column") |> 
  ggplot() + 
  geom_boxplot(aes(y = value)) + 
  facet_wrap(~Column, scales = "free")
```

It looks like we don't have any crazy outliers for Age, but we do have a single obvious outlier for Fare that should be removed. In order to drop the entire row, we have to join back the training set outcome (Survived) before our filter.

```{r}
#| label: removing Fare outlier

train <- data[data$train == "yes", ]
test <- data[data$train == "no", ]

train$Survived <- train.Y

train <- train |> 
  filter(Fare < 300)

train.Y <- train$Survived

train <- train |> 
  select(-Survived)

data <- rbind(train, test)
```

### Feature Engineering

Finally, let's perform some feature engineering. Considering the context of the Titanic, we know that children were more likely to survive. Therefore, let's create an indicator variable for child status.

```{r}
#| is.child indicator

data <- data |> 
  mutate(is.child = ifelse(Age < 16, "yes", "no"))
```

We also have some interesting variables in our dataset that could indicate family size (Parch and SibSp). Let's create a variable for family size. We will create a categorical variable to represent family size as small, medium, or large. We will also create another variable, is.alone, to indicate whether or not the given passenger is alone (i.e., has a family size of 1)

```{r}
#| family.size variable

data <- data |> 
  mutate(
    family.size = Parch + SibSp + 1, 
    family.size.cat = 
      case_when(
        family.size < 4 ~ "small", 
        family.size < 8 ~ "medium", 
        TRUE ~ "large"
      ),
    is.alone = ifelse(family.size == 1, "yes", "no")
  )
```

### Final Preprocessing

Finally, let's drop the variables we aren't interested in anymore, factorize any relevant variables, and split the data back into the training and testing sets. We also have to make sure that our outcome variable isn't numeric. 

```{r}
#| label: final preprocessing

## first, drop variables we aren't interested in anymore
data <- data |> 
  select(-c(Name, SibSp, Parch, Title, family.size))

## next, factorize the relevant variables
factor_var <- c("Pclass", "Sex", "Embarked", "is.child", "is.alone", "family.size.cat")

for(var in factor_var) {
  data[, var] <- factor(data[, var])
}

## split data back into training and testing sets
train <- data[data$train == "yes", ] |> 
  select(-train) |> 
  cbind(train.Y) |> 
  rename(Survived = train.Y)
test <- data[data$train == "no", ] |> 
  select(-train)

## make sure outcome variable isn't numeric
train <- train |> 
  mutate(Survived = ifelse(Survived == 1, "yes", "no"))
```

Now we're ready to move onto modeling.

## Statistical Modeling

First, let's define our resampling scheme.

```{r}
#| label: my_trControl

my_trControl <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
```

We will begin by fitting a few different logistic regression models. 

### Logistic Regression

```{r}
#| label: log_mod1

set.seed(15213)

log_mod1 <- train(Survived ~ ., 
                  data = train %>% select(-PassengerId), 
                  preProcess = c("center", "scale"),
                  trControl = my_trControl, 
                  method = "glm", 
                  family = binomial(link = "logit"),
                  metric = "Accuracy")

summary(log_mod1)
log_mod1
```

```{r}
#| label: log_mod2

set.seed(15213)

log_mod2 <- train(Survived ~ Pclass + Sex + Age + is.child, 
                  data = train %>% select(-PassengerId), 
                  trControl = my_trControl, 
                  method = "glm", 
                  family = binomial(link = "logit"),
                  metric = "Accuracy")

summary(log_mod2)
log_mod2
```


Now, let's try using elastic net regression.

### Elastic Net

```{r}
#| label: enet_mod1

set.seed(15213)

enet_mod1 <- train(Survived ~ ., 
                   data = train %>% select(-PassengerId), 
                   trControl = my_trControl, 
                   method = "glmnet", 
                   metric = "Accuracy")
```

Now, let's try using neural networks.

### Neural Networks

```{r}
#| label: nnet_mod1

set.seed(15213)

nnet_mod1 <- train(Survived ~ ., 
                   data = train %>% select(-PassengerId), 
                   preProcess = c("center", "scale"),
                   trControl = my_trControl, 
                   method = "nnet", 
                   metric = "Accuracy", 
                   trace = FALSE)
```

Next, let's try xgBoost.

### xgBoost

```{r}
#| label: xgbTree_mod1

set.seed(15213)

xgbTree_mod1 <- train(Survived ~ ., 
                      data = train %>% select(-PassengerId), 
                      preProcess = c("center", "scale"),
                      trControl = my_trControl, 
                      method = "xgbTree", 
                      metric = "Accuracy", 
                      verbosity = 0)
```

Finally, let's compare all of our models and choose the best one for predictions on the test set. 

```{r}
#| label: model comparison

model_comparison <- resamples(list(full_logistic = log_mod1, 
                                   partial_logistic = log_mod2, 
                                   elastic_net = enet_mod1,
                                   neural_network = nnet_mod1, 
                                   xgboost_tree = xgbTree_mod1))
                                   
  
dotplot(model_comparison, metric = 'Accuracy')
```

The xgbTree model performed best, as according to cross validated accuracy.

NOTE: I submitted predictions for all of the model types, and the partial logistic model ended up performing best!

### Predictions

```{r}
#| label: log_mod2 predictions

pred_log_mod2 <- predict(log_mod2, newdata = test, type = "prob")

log_output <- 
  tibble(
    PassengerId = test$PassengerId,
    Survived = ifelse(pred_log_mod2$yes < 0.5, 0, 1)
)
```

```{r}
#| label: nnet_mod1 predictions

pred_nnet_mod1 <- predict(nnet_mod1, newdata = test, type = "prob")

nnet_output <- 
  tibble(
    PassengerId = test$PassengerId,
    Survived = ifelse(pred_nnet_mod1$yes < 0.5, 0, 1)
)
```

```{r}
#| label: xgbTree_mod1 predictions

pred_xgbTree_mod1 <- predict(xgbTree_mod1, newdata = test, type = "prob")

xgbTree_output <- 
  tibble(
    PassengerId = test$PassengerId,
    Survived = ifelse(pred_xgbTree_mod1$yes < 0.5, 0, 1)
)
```

```{r}
#| label: write_csv

write.csv(log_output, file = "titanic_predictions.csv", row.names = FALSE)
```
